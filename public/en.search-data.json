{"/k0s/extensions_k0s/":{"data":{"":"Extensions allows to specify Helm charts or yaml manifests to be deployed during the creation or update of a cluster. We will illustrate the extensions capability on a single node cluster created with the k0s binary.","cleanup#Cleanup":"You can use the following command to remove the VM:\n$ multipass delete -p k0s-1","creation-of-a-vm#Creation of a VM":"First create a single VM, named k0s-1, using Multipass:\nmultipass launch -n k0s-1","extensions-with-helm-charts-in-k0s-configuration#Extensions with Helm Charts in k0s configuration":"Run a shell into the newly created VM:\n$ kubectl shell k0s-1 Next download k0s:\nubuntu@k0s-1:~$ curl -sSLf get.k0s.sh | sudo sh In the single node tutorial we talked about the default k0s configuration file which can be retrieved with the following command:\nubuntu@k0s-1:~$ k0s default-config \u003e k0s.yaml This file contains the default properties of all the cluster’s components, among them:\nthe API Server the ETCD key value store the network plugins … The following shows all the available properties\n$ cat k0s.yaml apiVersion: k0s.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s spec: api: address: 192.168.64.113 port: 6443 k0sApiPort: 9443 sans: - 192.168.64.113 storage: type: etcd etcd: peerAddress: 192.168.64.113 network: podCIDR: 10.244.0.0/16 serviceCIDR: 10.96.0.0/12 provider: kuberouter calico: null kuberouter: mtu: 0 peerRouterIPs: \"\" peerRouterASNs: \"\" autoMTU: true kubeProxy: disabled: false mode: iptables podSecurityPolicy: defaultPolicy: 00-k0s-privileged telemetry: enabled: true installConfig: users: etcdUser: etcd kineUser: kube-apiserver konnectivityUser: konnectivity-server kubeAPIserverUser: kube-apiserver kubeSchedulerUser: kube-scheduler images: konnectivity: image: us.gcr.io/k8s-artifacts-prod/kas-network-proxy/proxy-agent version: v0.0.21 metricsserver: image: gcr.io/k8s-staging-metrics-server/metrics-server version: v0.3.7 kubeproxy: image: k8s.gcr.io/kube-proxy version: v1.21.2 coredns: image: docker.io/coredns/coredns version: 1.7.0 calico: cni: image: docker.io/calico/cni version: v3.18.1 node: image: docker.io/calico/node version: v3.18.1 kubecontrollers: image: docker.io/calico/kube-controllers version: v3.18.1 kuberouter: cni: image: docker.io/cloudnativelabs/kube-router version: v1.2.1 cniInstaller: image: quay.io/k0sproject/cni-node version: 0.1.0 default_pull_policy: IfNotPresent konnectivity: agentPort: 8132 adminPort: 8133 Under the .spec property, add the following extensions property and its helm sub property:\nextensions: helm: repositories: - name: traefik url: https://helm.traefik.io/traefik charts: - name: traefik chartname: traefik/traefik version: \"9.11.0\" namespace: default This indicates to k0s that the Helm Chart for Traefik Ingress Controller needs to be installed during the bootstrap of the cluster.\nNext, install the cluster providing this configuration file:\nubuntu@k0s-1:~$ sudo k0s install controller --config ./k0s.yaml --single and start it:\nubuntu@k0s-1:~$ sudo k0s start Finally, make sure Traefik has been deployed to the cluster:\nubuntu@k0s-1:~$ sudo k0s kubectl get po -A You should get a result similar to the following one:\nNAMESPACE NAME READY STATUS RESTARTS AGE default traefik-1625417854-7f8d8d8987-p4z96 1/1 Running 0 2m58s kube-system coredns-5ccbdcc4c4-kb24b 1/1 Running 0 2m58s kube-system kube-proxy-t8jn9 1/1 Running 0 2m44s kube-system kube-router-vmhv7 1/1 Running 0 2m44s kube-system metrics-server-59d8698d9-259xl 1/1 Running 0 2m58s In order to verify Traefik has been installed via Helm, first install the helm client using the following command:\nubuntu@k0s-1:~$ curl -o helm.tar.gz https://get.helm.sh/helm-v3.6.2-linux-amd64.tar.gz \u0026\u0026 \\ tar -xvf helm.tar.gz \u0026\u0026 \\ sudo mv linux-amd64/helm /usr/local/bin Then run a root shell (that will make things easier in this example) and configure the helm client so it uses the kubeconfig file of the cluster:\nubuntu@k0s-1:~$ sudo su - root@k0s-1:~$ export KUBECONFIG=/var/lib/k0s/pki/admin.conf Finally, make sure Traefik Helm chart is listed:\nroot@k0s-1:~$ helm list You should get an output similar to the following one:\nNAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik-1625417854 default 1 2021-07-04 18:57:37.035554595 +0200 CEST deployed traefik-9.11.0 2.3.3 As you can see, using the extensions property makes it very easy to deploy additional application through Helm Chart.","extensions-with-yaml-manifests#Extensions with yaml manifests":"K0s also allows to deploy additional resources automatically by adding yaml manifests into the /var/lib/k0s/manifests/ folder.\nTo illustrate this, first create a mongo folder in /var/lib/k0s/manifests/ and create the following Deployment (based on the mongodb image) in the file mongo.yaml inside that new folder:\napiVersion: apps/v1 kind: Deployment metadata: name: db namespace: default spec: replicas: 1 selector: matchLabels: app: db template: metadata: creationTimestamp: null labels: app: db spec: containers: - image: mongo:4.4 name: mongo After a few tens of seconds, make sure the Deployment has been correctly created:\nubuntu@k0s-1:~$ sudo k0s kubectl get deploy,po -l app=db","pre-requisites#Pre-requisites":"Multipass"},"title":"extensions_k0s"},"/k0s/multi-nodes/multi_nodes/":{"data":{"":"In this part, you will create several virtual machines and run a multi node k0s on that ones","accessing-the-cluster#Accessing the cluster":"As you have done in the single_node tutorial, you need to:\nretrieve the kubeconfig file generated during the cluster installation multipass exec node-1 -- sudo cat /var/lib/k0s/pki/admin.conf \u003e kubeconfig modify that file to use the IP address of node-1 instead of localhost NODE1_IP=$(multipass info node-1 | grep IP | awk '{print $2}') sed -i '' \"s/localhost/$NODE1_IP/\" kubeconfig configure your local kubectl to use that file export KUBECONFIG=$PWD/kubeconfig You can now communicate with your cluster from your local machine:\nkubectl get nodes Note: the above list is empty… so it appears that your cluster does not have any node. That is not exactly true as only the worker nodes should be listed (those will be added in the next step), the controller node are just hidden. The isolation between the control plane components and the data plane ones is one great feature of k0s.\nIn the next step you will add worker nodes (nodes that will be used to run workload) in the cluster.","adding-some-worker-nodes#Adding some worker nodes":"First, you need to get a token from the control plane node:\nmultipass exec node-1 -- sudo k0s token create --role=worker \u003e ./worker_token Note: the worker role specified in the command indicates that the token will be used to add a worker (that is the default value). We could also use a controller role to add additional controllers in the cluster.\nNext copy that token into node-2 and node-3\nmultipass transfer ./worker_token node-2:/tmp/worker_token multipass transfer ./worker_token node-3:/tmp/worker_token Next install k0s onto node-2 and node-3:\nmultipass exec node-2 -- sudo k0s install worker --token-file /tmp/worker_token multipass exec node-3 -- sudo k0s install worker --token-file /tmp/worker_token Then start k0s on both worker nodes:\nmultipass exec node-2 -- sudo k0s start multipass exec node-3 -- sudo k0s start Listing the cluster’s nodes one more time, you should now be able to see the newly added workers:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION node-2 Ready 52s v1.23.5+k0s node-3 Ready 27s v1.23.5+k0s Note: it can take a few tens of seconds for the nodes to appear in Ready status\nAs you have seen, creating a multi nodes cluster is very simple with k0s. In a next tutorial you will use k0sctl, a k0s’s companion tool that makes this process even easier.","cleanup#Cleanup":"The following command delete the 3 VMs used in this tutorial\nmultipass delete -p node-1 node-2 node-3","create-the-virtual-machines#Create the virtual machines":"First run the following command to launch the VMs named node-1, node-2 and node-3:\nfor i in 1 2 3; do multipass launch -n node-$i done Next list the VMs and make sure they are running fine:\nmultipass list You should get an output similar to the following one:\nName State IPv4 Image node-1 Running 192.168.64.48 Ubuntu 20.04 LTS node-2 Running 192.168.64.49 Ubuntu 20.04 LTS node-3 Running 192.168.64.50 Ubuntu 20.04 LTS","get-the-k0s-binary#Get the k0s binary":"First, you need to get the *k0s$ binary on each VM:\nfor i in 1 2 3; do multipass exec node-$i -- bash -c \"curl -sSLf get.k0s.sh | sudo sh\" done","init-the-cluster#Init the cluster":"First install the k0s controller as a systemd unit file on node-1:\nmultipass exec node-1 -- sudo k0s install controller Next start the cluster:\nmultipass exec node-1 -- sudo k0s start Next make sure it is running fine:\nmultipass exec node-1 -- sudo systemctl status k0scontroller","pre-requisites#Pre-requisites":"Multipass: Multipass is a very handy tool which allows to create Ubuntu virtual machine in a very easy way. It is available on macOS, Windows and Linux."},"title":"multi_nodes"},"/k0s/multi-nodes/readme/":{"data":{"":"In this exercise you will create a muti node k0s cluster\nNote: node1 and node2 will be used, make sure to run each command on the correct node :)","adding-a-worker-node#Adding a worker node":"In this step you will add a worker node in the cluster\nFirst, from node1, get the join token:\nsudo k0s token create --role=worker Note: the worker role specified in the command indicates that the token will be used to add a worker (that is the default value). We could also use a controller role to add additional controllers in the cluster.\nNext copy that token into node2 in /tmp/worker_token.\nNote: to get a terminal or an editor on another VM, select the name of that VM in the list within the dropdown at the top of the screen\nNext download k0s onto node2:\ncurl -sSLf get.k0s.sh | sudo sh Install it as a worker node providing the join token as a parameter:\nsudo k0s install worker --token-file /tmp/worker_token Then start it:\nsudo k0s start It will take a few tens of seconds to get the worker node ready.\nMake sure it has started correctly\nsudo k0s status From node1, list the nodes:\nsudo k0s kubectl get node Note: only the worker node appears in this list due to the control plane isolation feature of k0s\nNAME STATUS ROLES AGE VERSION node2 Ready 32s v1.21.3+k0s You’ve succesfully created a multi-node cluster containing one controller and one worker node.","cleanup#Cleanup":"Remove k0s from node1 and node2 running the following commands on both VMs:\nsudo k0s stop sudo k0s reset","creating-a-controller-node#Creating a controller node":"In this part, make sure to run the commands on node1.\nAs you have done in the single node lab, first get the latest release of k0s:\ncurl -sSLf get.k0s.sh | sudo sh Next install k0s as a controller node:\nsudo k0s install controller Next start the newly created systemd service:\nsudo k0s start After a few seconds, verify it has been started properly:\nsudo k0s status You should should get an output similar the following one:\nVersion: v1.21.3+k0s.0 Process ID: 1343 Parent Process ID: 1 Role: controller+worker Init System: linux-systemd Service file: /etc/systemd/system/k0scontroller.service As k0s comes with its own kubectl subcommand, you can directly list the status of our single node cluster:\nsudo k0s kubectl get node Note: as the current node is a controller node only it will not be listed. You will get the following output instead.\nNo resources found"},"title":"README"},"/k0s/single-node/single_node_multipass/":{"data":{"":"In this part, you will create a VM and run k0s on that one.","cleanup#Cleanup":"Remove the ghost Deployment and Service:\nkubectl delete deploy/ghost svc/ghost In order to remove k0s from the VM (without deleting the VM) you first need to stop k0s:\nubuntu@node-1:~$ sudo k0s stop and then reset it:\nubuntu@node-1:~$ sudo k0s reset You can use the following command from test host machine if you need to remove the whole VM:\nmultipass delete -p node-1","communication-from-an-external-machine#Communication from an external machine":"As k0s comes with its own kubectl subcommand, you can communicate with the API Server directly from the master node:\nubuntu@node-1:~$ sudo k0s kubectl get node NAME STATUS ROLES AGE VERSION node-1 Ready control-plane 3m4s v1.23.5+k0s Usually, we do not ssh into a master node to run kubectl commands but use an admin machine instead. In order to do so, you first need to retrieve the kubeconfig file generated during the cluster creation (located in /var/lib/k0s/pki/admin.conf). Use the multipass’ helper for this purpose (the following command must be run from the host machine):\nmultipass exec node-1 -- sudo cat /var/lib/k0s/pki/admin.conf \u003e kubeconfig As this kubeconfig references an API Server on localhost, you need to get the IP address of the node-1 VM and use it in the kubeconfig file:\nNODE1_IP=$(multipass info node-1 | grep IP | awk '{print $2}') sed -i '' \"s/localhost/$NODE1_IP/\" kubeconfig Then configure your local kubectl so it uses this kubeconfig:\nexport KUBECONFIG=$PWD/kubeconfig You can now communicate with the newly created cluster from your machine. List the nodes to make sure this is working properly:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION node-1 Ready control-plane 4m57s v1.23.5+k0s","create-the-virtual-machine#Create the virtual machine":"First run the following command to launch a VM named node-1:\nmultipass launch -n node-1 Then run a shell on that VM\nmultipass shell node-1 Note: you will end up as the ubuntu user","default-k0s-configuration#Default k0s configuration":"When running a k0s cluster, the default configuration options are used but it is also possible to modify that one to better match specific needs.\nThe defaults configuration options can be retrieved with:\nk0s config create The output is similar to the following one:\napiVersion: k0s.k0sproject.io/v1beta1 kind: ClusterConfig metadata: creationTimestamp: null name: k0s spec: api: address: 192.168.64.78 k0sApiPort: 9443 port: 6443 sans: - 192.168.64.78 - fd6a:f620:1157:6e48:5054:ff:fe9d:8603 - fe80::5054:ff:fe9d:8603 tunneledNetworkingMode: false controllerManager: {} extensions: helm: charts: null repositories: null storage: create_default_storage_class: false type: external_storage images: calico: cni: image: docker.io/calico/cni version: v3.21.2 kubecontrollers: image: docker.io/calico/kube-controllers version: v3.21.2 node: image: docker.io/calico/node version: v3.21.2 coredns: image: k8s.gcr.io/coredns/coredns version: v1.7.0 default_pull_policy: IfNotPresent konnectivity: image: quay.io/k0sproject/apiserver-network-proxy-agent version: 0.0.30-k0s kubeproxy: image: k8s.gcr.io/kube-proxy version: v1.23.5 kuberouter: cni: image: docker.io/cloudnativelabs/kube-router version: v1.3.2 cniInstaller: image: quay.io/k0sproject/cni-node version: 0.1.0 metricsserver: image: k8s.gcr.io/metrics-server/metrics-server version: v0.5.2 installConfig: users: etcdUser: etcd kineUser: kube-apiserver konnectivityUser: konnectivity-server kubeAPIserverUser: kube-apiserver kubeSchedulerUser: kube-scheduler konnectivity: adminPort: 8133 agentPort: 8132 network: calico: null dualStack: {} kubeProxy: mode: iptables kuberouter: autoMTU: true mtu: 0 peerRouterASNs: \"\" peerRouterIPs: \"\" podCIDR: 10.244.0.0/16 provider: kuberouter serviceCIDR: 10.96.0.0/12 podSecurityPolicy: defaultPolicy: 00-k0s-privileged scheduler: {} storage: etcd: externalCluster: null peerAddress: 192.168.64.78 type: etcd telemetry: enabled: true status: {} :fire: to override some of those properties you can save the output of the previous command in a file, modify that one to match our needs and then use it when running k0s (more on that below)","get-the-k0s-binary#Get the k0s binary":"First, from the previous shell, get the latest release of k0s:\nubuntu@node-1:~$ curl -sSLf get.k0s.sh | sudo sh After a few tens of seconds the k0s binary will be available in your PATH (in /usr/local/bin).\nNext get the current version of the k0s binary:\nubuntu@node-1:~$ k0s version You should get an output similar to the following one (your version could be slightly different though)\nv1.23.5+k0s.0","install-k0s#Install k0s":"Once the k0s binary is installed, we can get a single node k0s cluster:\nubuntu@node-1:~$ sudo k0s install controller --single A systemd unit file has been created but the controller is not started yet:\nubuntu@node-1:~$ sudo systemctl status k0scontroller ● k0scontroller.service - k0s - Zero Friction Kubernetes Loaded: loaded (/etc/systemd/system/k0scontroller.service; enabled; vendor preset: enabled) Active: inactive (dead) Docs: https://docs.k0sproject.io You should get an output similar to the following one:\n:fire: if you need to provide some configuration options different from the default ones, you could provide a configuration file through the -c flag in the command above.","installation-using-a-cloud-config-file#Installation using a cloud config file":"In the steps above, you have:\ncreated a VM with multipass downloaded / installed / started k0s inside of it An alternative method to perform all those steps is to create the following cloud-config file:\n$ cat\u003c cloud-config.yaml #cloud-config runcmd: - curl -sSLf https://get.k0s.sh | sudo sh - sudo k0s install controller --single - sudo k0s start EOF and use it when creating the VM with Multipass:\nmultipass launch -n node-1 --cloud-init cloud-config.yaml","pre-requisites#Pre-requisites":"Multipass: Multipass is a very handy tool which allows to create Ubuntu virtual machine in a very easy way. It is available on macOS, Windows and Linux.\nhttps://kubernetes.io/docs/tasks/tools/#kubectl: kubectl is THE binary used to communicate with the API Server of a Kubernetes cluster. It can be used to manage the cluster and the workloads that are running inside of it.","start-the-cluster#Start the cluster":"First, start the cluster:\nubuntu@node-1:~$ sudo k0s start Next verify it has been started properly using the status subcommand:\nubuntu@node-1:~$ sudo k0s status You should should get an output similar the following one:\nVersion: v1.23.5+k0s.0 Process ID: 1843 Role: controller Workloads: true SingleNode: true It takes a few tens of seconds for the cluster to be up and running. In the following step you will configure your local kubectl binary to communicate with the cluster’s API Server.\nYou could also now see the k0scontroller is started in systemd:\n● k0scontroller.service - k0s - Zero Friction Kubernetes Loaded: loaded (/etc/systemd/system/k0scontroller.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2022-04-12 15:29:47 CEST; 52s ago Docs: https://docs.k0sproject.io Main PID: 1843 (k0s) Tasks: 101 Memory: 618.9M CGroup: /system.slice/k0scontroller.service ├─1843 /usr/local/bin/k0s controller --single=true ├─1859 /var/lib/k0s/bin/kine --endpoint=sqlite:///var/lib/k0s/db/state.db?more=rwc\u0026_journal=WAL\u0026cache=shared --l\u003e ├─1864 /var/lib/k0s/bin/kube-apiserver --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --api-au\u003e ├─1907 /var/lib/k0s/bin/kube-scheduler --profiling=false --authentication-kubeconfig=/var/lib/k0s/pki/scheduler.\u003e ├─1911 /var/lib/k0s/bin/kube-controller-manager --service-account-private-key-file=/var/lib/k0s/pki/sa.key --clu\u003e ├─1921 /var/lib/k0s/bin/containerd --root=/var/lib/k0s/containerd --state=/run/k0s/containerd --address=/run/k0s\u003e ├─1937 /var/lib/k0s/bin/kubelet --v=1 --node-labels=node.k0sproject.io/role=control-plane --container-runtime=re\u003e ├─2059 /var/lib/k0s/bin/containerd-shim-runc-v2 -namespace k8s.io -id 595dc4c6ab62c88cfd5b1fc352298c00c17f516829\u003e ├─2061 /var/lib/k0s/bin/containerd-shim-runc-v2 -namespace k8s.io -id d77e1aadb5657b0b9ee919021e441fea307b2c84be\u003e ├─2104 /pause └─2112 /pause ...","testing-the-whole-thing#Testing the whole thing":"Let’s now run a Deployment based on the ghost image (ghost is an open source blogging platform) and expose it though a NodePort Service.\ncat\u003c"},"title":"single_node_multipass"},"/k0sctl/backup_and_restore/":{"data":{"":"In this tutorial, you will:\ncreate 2 k0s clusters using k0sctl create a backup of the first one restore that backup in the second one :fire: both k0s and k0sctl have the backup / restore fontionnalities, in this tutorial we will use k0sctl to perform those operations.","backup-of-the-first-cluster#Backup of the first cluster":"First, configure your local kubectl with the kubeconfig file of the cluster k0s-1:\nexport KUBECONFIG=$PWD/kubeconfig-1 Before backing up the cluster, first launch a Pod so you can check if it’s correctly restored later on. Use the following imperative commands to run a Pod based on the ghost blogging platform and expose it with a Service:\n# Run a Pod based on Ghost kubectl run ghost --image=ghost:4 # Expose the pod kubectl expose --port 2368 pod/ghost Next, make sure the Pod and Service were correctly created:\n$ kubectl get pod,svc -l run=ghost NAME READY STATUS RESTARTS AGE pod/ghost 1/1 Running 0 8m15s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ghost ClusterIP 10.99.152.142 2368/TCP 6m59s Then use the following command to create a backup of the cluster with k0sctl:\n$ k0sctl backup --config cluster-k0s-1.yaml You should be an output similar to the following one:\n⠀⣿⣿⡇⠀⠀⢀⣴⣾⣿⠟⠁⢸⣿⣿⣿⣿⣿⣿⣿⡿⠛⠁⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀█████████ █████████ ███ ⠀⣿⣿⡇⣠⣶⣿⡿⠋⠀⠀⠀⢸⣿⡇⠀⠀⠀⣠⠀⠀⢀⣠⡆⢸⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀███ ███ ███ ⠀⣿⣿⣿⣿⣟⠋⠀⠀⠀⠀⠀⢸⣿⡇⠀⢰⣾⣿⠀⠀⣿⣿⡇⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀███ ███ ███ ⠀⣿⣿⡏⠻⣿⣷⣤⡀⠀⠀⠀⠸⠛⠁⠀⠸⠋⠁⠀⠀⣿⣿⡇⠈⠉⠉⠉⠉⠉⠉⠉⠉⢹⣿⣿⠀███ ███ ███ ⠀⣿⣿⡇⠀⠀⠙⢿⣿⣦⣀⠀⠀⠀⣠⣶⣶⣶⣶⣶⣶⣿⣿⡇⢰⣶⣶⣶⣶⣶⣶⣶⣶⣾⣿⣿⠀█████████ ███ ██████████ k0sctl v0.9.0 Copyright 2021, k0sctl authors. Anonymized telemetry of usage will be sent to the authors. By continuing to use k0sctl you agree to these terms: https://k0sproject.io/licenses/eula INFO ==\u003e Running phase: Connect to hosts INFO [ssh] 192.168.64.105:22: connected INFO [ssh] 192.168.64.104:22: connected INFO ==\u003e Running phase: Detect host operating systems INFO [ssh] 192.168.64.104:22: is running Ubuntu 20.04.2 LTS INFO [ssh] 192.168.64.105:22: is running Ubuntu 20.04.2 LTS INFO ==\u003e Running phase: Gather host facts INFO [ssh] 192.168.64.104:22: discovered enp0s2 as private interface INFO ==\u003e Running phase: Gather k0s facts INFO [ssh] 192.168.64.104:22: found existing configuration INFO [ssh] 192.168.64.104:22: is running k0s controller version 1.21.2+k0s.0 INFO [ssh] 192.168.64.105:22: is running k0s worker version 1.21.2+k0s.0 INFO [ssh] 192.168.64.104:22: checking if worker k0s-12 has joined INFO ==\u003e Running phase: Take backup INFO [ssh] 192.168.64.104:22: backing up INFO backup file written to /Users/luc/Development/k0s/k0s_backup_1625312160.tar.gz INFO ==\u003e Running phase: Disconnect from hosts INFO ==\u003e Finished in 8s As we can see in the output, we can see a tar.gz archive has been created. In the next step, we will restore this backup in the second cluster.","cleanup#Cleanup":"You can remove the 4 VMs using the following command:\n$ multipass delete -p k0s-11 k0s-12 k0s-21 k0s-22","creation-of-the-clusters#Creation of the clusters":"In this part, you will create 2 clusters:\nk0s-1 should contain 2 nodes\nk0s-11 a controller node k0s-12 a worker node k0s-2 should contain 2 nodes\nk0s-21 a controller node k0s-22 a worker node As you have done in some previous tutorials, you will first create a pair of ssh key that will be used in the next steps:\nssh-keygen -t rsa -q -N \"\" -f /tmp/k0s Next define a cloud.init file that references the public key\ncat\u003c cloud.init ssh_authorized_keys: - $(cat /tmp/k0s.pub) EOF Next create all the nodes using Multipass:\nmultipass launch -n k0s-11 --cloud-init cloud.init multipass launch -n k0s-12 --cloud-init cloud.init multipass launch -n k0s-21 --cloud-init cloud.init multipass launch -n k0s-22 --cloud-init cloud.init :Note: the –cloud-init flag allows to provide the cloud.init file to copy the ssh public key into each VM\nNext get the IP addresses of the VMs\nk0s11_IP=$(multipass info k0s-11 | grep IPv4 | awk '{print $2}') k0s12_IP=$(multipass info k0s-12 | grep IPv4 | awk '{print $2}') k0s21_IP=$(multipass info k0s-21 | grep IPv4 | awk '{print $2}') k0s22_IP=$(multipass info k0s-22 | grep IPv4 | awk '{print $2}') Next create a cluster configuration file for each cluster:\n# Cluster k0s-1 k0sctl init -i /tmp/k0s -C 1 ubuntu@${k0s11_IP} ubuntu@${k0s12_IP} \u003e cluster-k0s-1.yaml # Cluster k0s-2 k0sctl init -i /tmp/k0s -C 1 ubuntu@${k0s21_IP} ubuntu@${k0s22_IP} \u003e cluster-k0s-2.yaml Next create the clusters:\n# Cluster k0s-1 k0sctl apply --config cluster-k0s-1.yaml # Cluster k0s-2 k0sctl apply --config cluster-k0s-2.yaml Then get the kubeconfig files of each one:\n# Cluster k0s-1 k0sctl kubeconfig --config cluster-k0s-1.yaml \u003e kubeconfig-1 # Cluster k0s-2 k0sctl kubeconfig --config cluster-k0s-2.yaml \u003e kubeconfig-2","pre-requisites#Pre-requisites":"Multipass K0sctl","restore-the-backup#Restore the backup":"First, configure your local kubectl with the kubeconfig file of the cluster k0s-2:\nexport KUBECONFIG=$PWD/kubeconfig-2 Next make sure no Pod nor Service exist in the default namespace:\n$ kubectl get pod,svc Only the default internal kubernetes service should be listed:\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 443/TCP 15m Then use the following command to restore the previous backup with k0sctl:\n:fire: use the path towards the tar.gz file provided at the end of the backup step\n$ k0sctl apply --config cluster-k0s-2.yaml --restore-from /Users/luc/Development/k0s/k0s_backup_1625312160.tar.gz :fire: this restore step is not working yet, I’m investigating what is missing in my configuration"},"title":"backup_and_restore"},"/k0sctl/upgrade_a_cluster/":{"data":{"":"In this tutorial, you will first create a cluster using k0sctl and then upgrade it to a newer version of Kubernetes. As you have already done in previous tutorials, you will use Multipass to create local virtual machines.","cleanup#Cleanup":"The following command removes all the k0s related components but keeps the VMs:\n$ k0sctl reset -c cluster.yaml In case you want to remove all the VMs as well you can directly run the following command:\n$ multipass delete -p node-1 node-2 node-3 node-4 node-5","cluster-configuration#Cluster configuration":"First you will generate a sample cluster configuration file, this can be done with the following command:\n$ k0sctl init \u003e cluster.yaml This command returns a content similar to the following one:\napiVersion: k0sctl.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s-cluster spec: hosts: - ssh: address: 10.0.0.1 user: root port: 22 keyPath: /Users/luc/.ssh/id_rsa role: controller - ssh: address: 10.0.0.2 user: root port: 22 keyPath: /Users/luc/.ssh/id_rsa role: worker k0s: version: 1.21.2+k0s.0 Next, modify that file so it defines:\nnode-1, node-2 and node-3 as controller nodes node-4, node-5 as worker nodes k0s 1.20.6 as the cluster version (this version is not the latest one) Using the address IP from the previous command, the modified version of the cluster.yaml file would look like the following:\napiVersion: k0sctl.k0sproject.io/v1beta1 kind: Cluster metadata: name: k0s-cluster spec: hosts: - ssh: address: 192.168.64.11 user: ubuntu port: 22 keyPath: /tmp/k0s role: controller - ssh: address: 192.168.64.12 user: ubuntu port: 22 keyPath: /tmp/k0s role: controller - ssh: address: 192.168.64.13 user: ubuntu port: 22 keyPath: /tmp/k0s role: controller - ssh: address: 192.168.64.29 user: ubuntu port: 22 keyPath: /tmp/k0s role: worker - ssh: address: 192.168.64.30 user: ubuntu port: 22 keyPath: /tmp/k0s role: worker k0s: version: 1.20.6+k0s.0","create-the-virtual-machines#Create the virtual machines":"First, create a new pair of ssh key:\n$ ssh-keygen -t rsa -q -N \"\" -f /tmp/k0s Next create the cloud.init file that will contains the public key:\ncat\u003c cloud.init ssh_authorized_keys: - $(cat /tmp/k0s.pub) EOF Run the following command to launch the VMs named node-1 to node-5:\nNote: the –cloud-init flag is used to configure each VM with the public key created in the previous step\nfor i in $(seq 1 5); do multipass launch -n node-$i --cloud-init cloud.init done Make sure those VMs are running fine:\n$ multipass list You should get an output similar to the following one (the IP addresses would probably be different though):\nName State IPv4 Image node-1 Running 192.168.64.11 Ubuntu 20.04 LTS node-2 Running 192.168.64.12 Ubuntu 20.04 LTS node-3 Running 192.168.64.13 Ubuntu 20.04 LTS node-4 Running 192.168.64.29 Ubuntu 20.04 LTS node-5 Running 192.168.64.30 Ubuntu 20.04 LTS You will use the IP addresses of those machines in a next step.","lauching-the-cluster#Lauching the cluster":"Once the cluster’s configuration file is ready, you only need to run the following command to create the cluster (it only takes a couple of minutes for the cluster to be up and running):\n$ k0sctl apply --config cluster.yaml Next you can retrieve the cluster’s kubeconfig file and configure your local kubectl using that one:\nk0sctl kubeconfig -c cluster.yaml \u003e kubeconfig export KUBECONFIG=$PWD/kubeconfig You should now be able to list the cluster’s nodes and get an output similar to the following one:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION node-4 Ready 76s v1.20.6-k0s1 node-5 Ready 77s v1.20.6-k0s1 Note: only the worker nodes are listed as the controller ones are isolated for a better security\nAs you can see, the nodes are in the version provided in the .spec.k0s.version property.","pre-requisites#Pre-requisites":"Multipass K0sctl","upgrade#Upgrade":"In order to upgrade the cluster to a newer version (1.21.1 in this example), change the cluster configuration file with the desired value in .spec.k0s.version:\n... k0s: version: 1.21.1+k0s.0 You can then upgrade the cluster with the same command as the one used to create it:\n$ k0sctl apply --config cluster.yaml During the process, we can see k0sctl get the current status of each node checking if an upgrade is needed:\n... INFO ==\u003e Running phase: Gather k0s facts INFO [ssh] 192.168.64.13:22: found existing configuration INFO [ssh] 192.168.64.11:22: found existing configuration INFO [ssh] 192.168.64.11:22: is running k0s controller version 1.20.6+k0s.0 WARN [ssh] 192.168.64.11:22: k0s will be upgraded INFO [ssh] 192.168.64.12:22: is running k0s controller version 1.20.6+k0s.0 WARN [ssh] 192.168.64.12:22: k0s will be upgraded INFO [ssh] 192.168.64.13:22: is running k0s controller version 1.20.6+k0s.0 WARN [ssh] 192.168.64.13:22: k0s will be upgraded INFO [ssh] 192.168.64.29:22: is running k0s worker version 1.20.6+k0s.0 WARN [ssh] 192.168.64.29:22: k0s will be upgraded INFO [ssh] 192.168.64.11:22: checking if worker node4 has joined INFO [ssh] 192.168.64.30:22: is running k0s worker version 1.20.6+k0s.0 WARN [ssh] 192.168.64.30:22: k0s will be upgraded INFO [ssh] 192.168.64.11:22: checking if worker node5 has joined ... k0sctl will then upgrade all the nodes starting with the controller ones. It only takes a couple of minutes for the cluster to be upgraded.\nYou should now be able to list the cluster’s nodes and verify they are in the updated version:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION node-4 Ready 16m v1.21.1-k0s1 node-5 Ready 16m v1.21.1-k0s1"},"title":"upgrade_a_cluster"}}